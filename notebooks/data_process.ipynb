{
 "metadata": {
  "name": "",
  "signature": "sha256:4e80a5370cc19b6f5a7d2a8f55a67f41d5ab39b41f3f089cc75ac2db3635289c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "\n",
      "data_path = 'C:/Users/mramire8/Documents/Datasets/twitter'\n",
      "data_path = '../../data/twitter'\n",
      "\n",
      "def get_tweets_file(path):\n",
      "    f = open(path)\n",
      "\n",
      "    i = 0\n",
      "    users = []\n",
      "    for line in f:\n",
      "        data = line.split(\"]][[\")\n",
      "        last = len(data)\n",
      "\n",
      "    for i,tweets in enumerate(data):\n",
      "            if i == 0:\n",
      "                t = json.loads(tweets[1:] + \"]\")\n",
      "            elif i == (last-1):\n",
      "                t = json.loads(\"[\"+tweets[:-1])\n",
      "            else:\n",
      "                t = json.loads(\"[\"+tweets+\"]\")\n",
      "            users.append(t)\n",
      "\n",
      "    return users\n",
      "\n",
      "good = get_tweets_file(data_path + \"/good.json\")\n",
      "print \"Real users %s\" % (len(good))\n",
      "     \n",
      "bots = get_tweets_file(data_path + \"/bots.json\")\n",
      "print \"Bot users %s\" % (len(bots))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Real users 883\n",
        "Bot users 898"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"total: \", 883+898\n",
      "print \"\\n\".join(sorted(good[0][0].keys()))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "total:  1781\n",
        "contributors\n",
        "coordinates\n",
        "created_at\n",
        "entities\n",
        "favorite_count\n",
        "favorited\n",
        "geo\n",
        "id\n",
        "id_str\n",
        "in_reply_to_screen_name\n",
        "in_reply_to_status_id\n",
        "in_reply_to_status_id_str\n",
        "in_reply_to_user_id\n",
        "in_reply_to_user_id_str\n",
        "lang\n",
        "place\n",
        "possibly_sensitive\n",
        "retweet_count\n",
        "retweeted\n",
        "source\n",
        "text\n",
        "truncated\n",
        "user\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"\\n\".join(sorted(good[0][0]['user'].keys()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "contributors_enabled\n",
        "created_at\n",
        "default_profile\n",
        "default_profile_image\n",
        "description\n",
        "entities\n",
        "favourites_count\n",
        "follow_request_sent\n",
        "followers_count\n",
        "following\n",
        "friends_count\n",
        "geo_enabled\n",
        "id\n",
        "id_str\n",
        "is_translation_enabled\n",
        "is_translator\n",
        "lang\n",
        "listed_count\n",
        "location\n",
        "name\n",
        "notifications\n",
        "profile_background_color\n",
        "profile_background_image_url\n",
        "profile_background_image_url_https\n",
        "profile_background_tile\n",
        "profile_banner_url\n",
        "profile_image_url\n",
        "profile_image_url_https\n",
        "profile_link_color\n",
        "profile_location\n",
        "profile_sidebar_border_color\n",
        "profile_sidebar_fill_color\n",
        "profile_text_color\n",
        "profile_use_background_image\n",
        "protected\n",
        "screen_name\n",
        "statuses_count\n",
        "time_zone\n",
        "url\n",
        "utc_offset\n",
        "verified\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Distribution of dates from Tweets\n",
      "To avoid bias of the classifier by trending topics the data should belong to the same date range. We want to make sure the users belong to the same period of time. \n",
      "\n",
      "For example, the classifier can learn the topics instead of distinguishing bots from real users. We check for the date distribution of both classes. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime\n",
      "from collections import Counter\n",
      "\n",
      "def get_date(date_str):\n",
      "    return datetime.datetime.strptime(date_str.strip('\"'), \"%a %b %d %H:%M:%S +0000 %Y\")\n",
      "\n",
      "# datetime.strptime((r.json()[x][\"created_at\"]).strip('\"'), \"%a %b %d %H:%M:%S +0000 %Y\")\n",
      "def count_dates(users):\n",
      "    dates = Counter()\n",
      "    min_dt = get_date(users[0][0]['created_at'])\n",
      "    for user in users:\n",
      "        d = get_date(user[0]['created_at'])\n",
      "        min_dt = min(min_dt, d)\n",
      "        dates.update([d])\n",
      "    return dates, min_dt\n",
      "\n",
      "good_counts, min_good = count_dates(good)\n",
      "print \"Most common: %s\" % good_counts.most_common(3)\n",
      "print \"Latest: %s\" % min_good\n",
      "        \n",
      "bots_counts, min_bots = count_dates(bots)\n",
      "print \"Most common: %s\" % bots_counts.most_common(3)\n",
      "print \"Latest: %s\" % min_bots\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Most common: [(datetime.datetime(2014, 10, 29, 2, 27, 39), 2), (datetime.datetime(2014, 10, 29, 1, 30, 20), 2), (datetime.datetime(2014, 10, 28, 23, 0, 55), 2)]\n",
        "Latest: 2009-11-24 02:11:19\n",
        "Most common: [(datetime.datetime(2014, 10, 29, 6, 46, 9), 2), (datetime.datetime(2014, 10, 30, 0, 17, 5), 2), (datetime.datetime(2014, 10, 20, 22, 24, 23), 1)]\n",
        "Latest: 2009-11-01 12:56:32\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Number of users that have old tweets in good users\n",
      "## with tweets not in 2014\n",
      "print \"Old good users %s\" %  len([d for d in good_counts.keys() if d.year < 2014])\n",
      "print \"Old bot users %s\" %  len([d for d in bots_counts.keys() if d.year < 2014])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Old good users 45\n",
        "Old bot users 173\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.dates as mdates\n",
      "import matplotlib as mpl \n",
      "\n",
      "# mpl.style.use('fivethirtyeight')\n",
      "\n",
      "years    = mdates.YearLocator()   # every year\n",
      "months   = mdates.MonthLocator()  # every month\n",
      "yearsFmt = mdates.DateFormatter('%Y')\n",
      "monthsFmt = mdates.DateFormatter('%Y-%m')\n",
      "fig = plt.figure()\n",
      "ax = plt.axes()\n",
      "\n",
      "gds =[(d,c) for d,c in good_counts.iteritems() if d.year > 2013]\n",
      "bts =[(d,c) for d,c in bots_counts.iteritems() if d.year > 2013]\n",
      "\n",
      "\n",
      "kg = [d.toordinal() for d,_ in gds]\n",
      "kb = [d.toordinal() for d,_ in bts]\n",
      "wg = [c for _,c in gds]\n",
      "wb = [c for _,c in bts]\n",
      "plt.hist([kg,kb], weights=[wg,wb], bins=20, stacked=False, alpha=.7, label=['real', 'bots'])\n",
      "\n",
      "ax.xaxis.set_major_locator(months)\n",
      "ax.xaxis.set_major_formatter(monthsFmt)\n",
      "# ax.xaxis.set_minor_locator(months)\n",
      "plt.legend(loc='best', frameon=False, numpoints=1)\n",
      "ax.format_xdata = mdates.DateFormatter('%Y-%m-%d')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10f1aa590>"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Data Processing\n",
      "For each user we have tweet in the timeline. Let $s=\\{t | t \\text{ is a tweet}\\}_{1}^{200}$ be the tweets of a user. Let $D=\\{(s,y)\\}_1^n$, where $y=\\{'human','bot'\\}$. \n",
      "\n",
      "### Text Processing \n",
      "For every user we process the text of the twee as follows:\n",
      "* Collapse URL\n",
      "* Collapse mentions\n",
      "* Lower case all text \n",
      "* Remove uses that have not tweeted this year (i.e., 2014)\n",
      "\n",
      "###Dataset Format\n",
      "We create a dataset dictionary containing: \n",
      "* **data**: All tweet objects per user\n",
      "* **target**: Class label of each user. Labels are 0:humans 1:bots\n",
      "* **user_id**: screen name of the user\n",
      "* **user_name**: full name of the user\n",
      "* **user_text**: all text of tweets converted into one single text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## convert the tweet into a data format of text documents\n",
      "# from sklearn.datasets.base import Bunch\n",
      "import re\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "\n",
      "def preprocess(string, lowercase, collapse_urls, collapse_mentions):\n",
      "    if not string:\n",
      "        return \"\"\n",
      "    if lowercase:\n",
      "        string = string.lower()\n",
      "#     tokens = []\n",
      "    if collapse_urls:\n",
      "        string = re.sub('http\\S+', 'THIS_IS_A_URL', string)\n",
      "    if collapse_mentions:\n",
      "        string = re.sub('@\\S+', 'THIS_IS_A_MENTION', string)\n",
      "#     if prefix:\n",
      "#         tokens = ['%s%s' % (prefix, t) for t in tokens]\n",
      "    return string\n",
      "\n",
      "def timeline_to_doc(user, *args):\n",
      "    tweets = []\n",
      "    for tw in user:\n",
      "        tweets.append(preprocess(tw['text'], *args))\n",
      "    return tweets\n",
      "\n",
      "def user_to_doc(users, *args):\n",
      "    timeline = []\n",
      "    user_names = []\n",
      "    user_id = []\n",
      "    \n",
      "    for user in users:\n",
      "        timeline.append(timeline_to_doc(user, *args))\n",
      "        user_names.append(user[0]['user']['name'])\n",
      "        user_id.append(user[0]['user']['screen_name'])\n",
      "    return user_id, user_names, timeline\n",
      "\n",
      "def bunch_users(class1, class2, vct, *options):\n",
      "    labels = None\n",
      "    if labels is None:\n",
      "        labels = [0,1]\n",
      "\n",
      "    user_id, user_names, timeline = user_to_doc(class1, *options)\n",
      "    user_id2, user_names2, timeline2 = user_to_doc(class2, *options)\n",
      "    target = [labels[0]] * len(user_id)\n",
      "    user_id.extend(user_id2)\n",
      "    user_names.extend(user_names2)\n",
      "    timeline.extend(timeline2)\n",
      "    target.extend([labels[1]]* len(user_id2))\n",
      "    user_text = [\". \".join(t) for  t in timeline]\n",
      "#     data = Bunch(data=timeline, target=target, user_id=user_id, user_name=user_names)\n",
      "    data = {'data':timeline, 'target':np.array(target), 'user_id':user_id, 'user_name':user_names, 'user_text':user_text}\n",
      "    data['bow'] = vct.fit_transform(data['user_text'])\n",
      "\n",
      "    random_state = np.random.RandomState(5612)        \n",
      "\n",
      "    indices = np.arange(data['bow'].shape[0])\n",
      "    random_state.shuffle(indices)\n",
      "    data['target'] = np.array(data['target'])[indices]\n",
      "    data_lst = np.array(data['data'] , dtype=object)\n",
      "    data_lst = data_lst[indices]\n",
      "    data['data'] = data_lst.tolist()\n",
      "    data['bow'] = data['bow'][indices]\n",
      "    data['user_id'] = np.array(data['user_id'])[indices]\n",
      "    data['user_name'] = np.array(data['user_id'])[indices]\n",
      "    data['user_text'] = np.array(data['user_id'])[indices]\n",
      "\n",
      "    return data\n",
      "       "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vct = TfidfVectorizer(encoding='latin1', min_df=5, max_df=1.0, binary=False, ngram_range=(1, 1),\n",
      "                      token_pattern='\\\\b\\\\w+\\\\b') \n",
      "\n",
      "gds =[g for g in good if get_date(g[0]['created_at']).year > 2013]\n",
      "bts =[b for b in bots if get_date(b[0]['created_at']).year > 2013]\n",
      "\n",
      "data = bunch_users(gds,bts, vct, True, False, False)\n",
      "\n",
      "print \"Total data:\", len(data['target'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "global name 'np' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-8-2f8f76347394>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbts\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbots\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mget_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created_at'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2013\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbunch_users\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Total data:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-7-c17803d94f67>\u001b[0m in \u001b[0;36mbunch_users\u001b[0;34m(class1, class2, vct, *options)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0muser_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\". \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m  \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtimeline\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#     data = Bunch(data=timeline, target=target, user_id=user_id, user_name=user_names)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtimeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'user_id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'user_name'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0muser_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'user_text'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0muser_text\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bow'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: global name 'np' is not defined"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Learning Curve: Random Sampling Baseline\n",
      "\n",
      "Test the learning curve as a baseline. This curve is of a classifier trained on documents. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import os\n",
      "\n",
      "sys.path.append(os.path.abspath(\"/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\"))\n",
      "\n",
      "from sklearn.learning_curve import learning_curve\n",
      "from sklearn import linear_model\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "# import brewer2mpl\n",
      "from sklearn.cross_validation import StratifiedKFold, cross_val_score, KFold, ShuffleSplit\n",
      "import itertools\n",
      "\n",
      "def learning_curve_tweet(data,clf, sizes=None):\n",
      "\n",
      "    col = brewer2mpl.get_map('Set1', 'qualitative', 7).mpl_colors\n",
      "    colors_n = itertools.cycle(col)\n",
      "\n",
      "    indices = np.arange(data['bow'].shape[0])\n",
      "    random_state.shuffle(indices)\n",
      "    data['target'] = np.array(data['target'])[indices]\n",
      "    data_lst = np.array(data['data'] , dtype=object)\n",
      "    data_lst = data_lst[indices]\n",
      "    data['data'] = data_lst.tolist()\n",
      "    data['bow'] = data['bow'][indices]\n",
      "    try:\n",
      "        data['user_id'] = np.array(data['user_id'])[indices]\n",
      "        data['user_name'] = np.array(data['user_id'])[indices]\n",
      "        data['user_text'] = np.array(data['user_id'])[indices]\n",
      "    except Exception:\n",
      "        pass\n",
      "\n",
      "    kcv = KFold(len(data['target']), n_folds=5, random_state=random_state,shuffle=True)\n",
      "\n",
      "    scoring_fn = 'accuracy'\n",
      "#     print(\"Classifier name:\", clf.__class__.__name__, \"C=\", clf.C)\n",
      "#     print(\"CV data:\", data['bow'])\n",
      "    if sizes is None:\n",
      "        sizes = range(20, 4* len(data['target'])/5, 100)\n",
      "    train_sizes, train_scores, test_scores = learning_curve(\n",
      "        clf, data['bow'], data['target'], train_sizes=sizes, cv=5, scoring=scoring_fn, n_jobs=2)\n",
      "    \n",
      "    current_color = colors_n.next()\n",
      "\n",
      "    train_scores_mean = np.mean(train_scores, axis=1)\n",
      "    train_scores_std = np.std(train_scores, axis=1)\n",
      "    test_scores_mean = np.mean(test_scores, axis=1)\n",
      "    test_scores_std = 1.0 * np.std(test_scores, axis=1) / np.sqrt(5.0)\n",
      "\n",
      "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
      "                     test_scores_mean + test_scores_std, alpha=0.1, color=current_color)\n",
      "    plt.plot(train_sizes, test_scores_mean, 'o-', mfc='white', linewidth=2, mew=2, markersize=10, mec=current_color, color=current_color,\n",
      "             # label=\"Cross-validation score\")\n",
      "             label=\"C={}\".format(clf.C))\n",
      "\n",
      "    print (\"-\"*40)\n",
      "    print (\"\\nCOST\\tMEAN\\tSTDEV\")\n",
      "    print (\"\\n\".join([\"{0}\\t{1:.3f}\\t{2:.4f}\".format(c,m,s) for c,m,s in zip(train_sizes, test_scores_mean, test_scores_std)]))\n",
      "    plt.legend(loc=\"best\")\n",
      "    # plt.savefig('lr-{0}.png'.format(vct.__class__.__name__), bbox_inches=\"tight\", dpi=200, transparent=True)\n",
      "    plt.savefig('lradapt-sent-sent.png', bbox_inches=\"tight\", dpi=200, transparent=True)\n",
      "    plt.show()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = 'lr'\n",
      "if classifier == \"mnb\":\n",
      "    clf = MultinomialNB(alpha=1)\n",
      "else:\n",
      "    clf = linear_model.LogisticRegression(penalty='l1', C=10)\n",
      "\n",
      "learning_curve_tweet(data,clf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Preprocessing Options\n",
      "The following results show how the preprocessing of text affects the classification\n",
      "\n",
      "###Results\n",
      "**Text Processing**\n",
      "We observe that there are not significant in the options. However, the best options are:\n",
      "* lower=True\turl=True\tmention=True\n",
      "* lower=True\turl=False\tmention=True\n",
      "* lower=False\turl=True\tmention=True\n",
      "* lower=False\turl=False\tmention=True\n",
      "\n",
      "This seems to suggest that collapsing mentions helps more than other processing options.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## try all combinations of data\n",
      "\n",
      "def try_all(clf, vct, good, bots):\n",
      "    # Trying all possible options \n",
      "    lowercase_opts = [True, False]\n",
      "    # keep_punctuation_opts = [True, False]\n",
      "    url_opts = [True, False]\n",
      "    mention_opts = [True, False]\n",
      "\n",
      "    argnames = ['lower', 'url', 'mention']\n",
      "    option_iter = itertools.product( lowercase_opts,\n",
      "                           url_opts,\n",
      "                           mention_opts)\n",
      "    results = []\n",
      "    for options in option_iter:\n",
      "        print '\\t'.join('%s=%s' % (name, opt) for name, opt in zip(argnames, options))\n",
      "        data = bunch_users(good, bots, vct, *options)\n",
      "        cv_scores = cross_val_score(clf, data['bow'], data['target'], cv=5, n_jobs=1)\n",
      "        print(\"5-f CV Accuracy: %0.2f (+/- %0.2f)\" % (cv_scores.mean(), cv_scores.std() * 2))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = linear_model.LogisticRegression(penalty='l1', C=10)\n",
      "\n",
      "vct = TfidfVectorizer(encoding='latin1', min_df=5, max_df=1.0, binary=False, ngram_range=(1,1),\n",
      "                      token_pattern='\\\\b\\\\w+\\\\b') \n",
      "\n",
      "try_all(clf, vct, gds, bts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_features(coef, names):\n",
      "    \"\"\" Print sorted list of non-zero features/weights. \"\"\"\n",
      "    ### coef = clf.coef_[0]\n",
      "    ### names = vec.get_feature_names()\n",
      "    print \"*\" * 50\n",
      "    print(\"Number of Features: %s\" % len(names))\n",
      "    print \"\\n\".join('%s\\t%.2f' % (names[j], coef[j]) for j in np.argsort(coef)[::-1] if coef[j] != 0)\n",
      "    print \"*\" * 50\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = linear_model.LogisticRegression(penalty='l1', C=10)\n",
      "data = bunch_users(gds, bts, vct, True, True, True)\n",
      "clf.fit(data['bow'], data['target'])\n",
      "print_features(clf.coef_[0], vct.get_feature_names())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Classifier Parameters: Grid Search\n",
      "\n",
      "For the classifier of choice we look for the best possible paramter to maximize accuracy. For logistic regression we look for the C penalty of regularization.\n",
      "\n",
      "### Results: \n",
      "We found that C=10 seems to work well with the data. \n",
      "\n",
      "*Note:* The data only includes users with tweets in the current year. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Grid search best estimator\n",
      "from sklearn.cross_validation import StratifiedKFold, train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "def grid_search_clf(data):\n",
      "    tuned_parameters = [{'C':  [pow(10,x) for x in range(-3,4)]}]\n",
      "    # scores = ['accuracy','precision', 'recall']\n",
      "    measure = 'accuracy'\n",
      "    X_train, X_test, y_train, y_test = train_test_split(data['bow'], data['target'], test_size=0.25, random_state=0)\n",
      "    kcv = StratifiedKFold(y=y_train, n_folds=5, shuffle=True, random_state=546321)\n",
      "    \n",
      "    print(\"# Tuning hyper-parameters for %s\" % measure)\n",
      "    print(len(y_train))\n",
      "    clf_new = linear_model.LogisticRegression(penalty='l1')\n",
      "    clf = GridSearchCV(clf_new, tuned_parameters, cv=5, scoring=measure, n_jobs=10)\n",
      "    clf.fit(X_train, y_train)\n",
      "\n",
      "    print(\"Best parameters set found on development set:\")\n",
      "    print(clf.best_estimator_)\n",
      "    print()\n",
      "    print(\"Grid scores on development set:\")\n",
      "    print()\n",
      "    for params, mean_score, scores in clf.grid_scores_:\n",
      "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
      "              % (mean_score, scores.std() / 2, params))\n",
      "    print()\n",
      "\n",
      "    print(\"Detailed classification report:\")\n",
      "    print()\n",
      "    print(\"The model is trained on the full development set.\")\n",
      "    print(\"The scores are computed on the full evaluation set.\")\n",
      "    print()\n",
      "    y_true, y_pred = y_test, clf.predict(X_test)\n",
      "    print(classification_report(y_true, y_pred))\n",
      "    print()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grid_search_clf(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Data Representation\n",
      "We test the effect of the data representation in the performance of the classifier.\n",
      "\n",
      "###Results\n",
      "We observe that the performance of the classifier does not increase significantly. \n",
      "\n",
      "**Data Representation**\n",
      "\n",
      "We tried two vectorizers: Count and TFIdf. We combined with unigrams and two-grams. \n",
      "\n",
      "TFIDF + one-grams and two-grams work better. \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vct = TfidfVectorizer(encoding='latin1', min_df=5, max_df=1.0, binary=False, ngram_range=(1,2),\n",
      "                      token_pattern='\\\\b\\\\w+\\\\b') \n",
      "\n",
      "try_all(clf, vct, gds, bts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vct = TfidfVectorizer(encoding='latin1', min_df=1, max_df=1.0, binary=False, ngram_range=(1,1),\n",
      "                      token_pattern='\\\\b\\\\w+\\\\b') \n",
      "\n",
      "try_all(clf, vct, gds, bts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vct = CountVectorizer(encoding='latin1', min_df=5, max_df=1.0, binary=False, ngram_range=(1,1),\n",
      "                      token_pattern='\\\\b\\\\w+\\\\b') \n",
      "\n",
      "try_all(clf, vct, gds, bts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Testing Sentence Classifier\n",
      "Test how does a sentence classifier do classifying into bots and humans."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##sentence detector in tweets\n",
      "def sentence_detector(timeline):\n",
      "    tl = []\n",
      "    print len(timeline)\n",
      "    for tw in timeline:\n",
      "        tl.append(tw)\n",
      "    return tl\n",
      "\n",
      "def convert2sentence(data):\n",
      "    all_sent=[]\n",
      "    all_target=[]\n",
      "    for user_timeline, label in zip(data['data'], data['target']):\n",
      "        sentences = user_timeline\n",
      "        lbls = [label] * len(sentences)\n",
      "        all_sent.extend(sentences)\n",
      "        all_target.extend(lbls)\n",
      "    return all_sent, all_target\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vct = TfidfVectorizer(encoding='latin1', min_df=5, max_df=1.0, binary=False, ngram_range=(1,2),\n",
      "                      token_pattern='\\\\b\\\\w+\\\\b') \n",
      "x_sent,y_sent = convert2sentence(data)\n",
      "\n",
      "indices = np.arange(len(y_sent))\n",
      "random_state.shuffle(indices)\n",
      "y_sent = np.array(y_sent)[indices]\n",
      "data_lst = np.array(x_sent , dtype=object)\n",
      "data_lst = data_lst[indices]\n",
      "x_sent = data_lst.tolist()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_sent = linear_model.LogisticRegression(penalty='l1', C=10)\n",
      "print \"Total sentences:\", len(y_sent)\n",
      "learning_curve_tweet({'data':x_sent, 'target':y_sent, 'bow':vct.fit_transform(x_sent)},clf, sizes=range(1000, 20000, 1000))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Other Features\n",
      "Term uniqueness? \n",
      "\n",
      "%%latex\n",
      "\\begin{align}\n",
      "f(x) = \\frac{TF}{#UniqueTerms}\n",
      "\\begin{align}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}