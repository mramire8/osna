{
 "metadata": {
  "name": "",
  "signature": "sha256:4162d9d77f2b575fb5839a2f9d38ca9011804c1dd6515524014aba4a1febd2f0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "\n",
      "data_path = 'C:/Users/mramire8/Documents/Datasets/twitter'\n",
      "data_path = '../../data/twitter'\n",
      "\n",
      "def get_tweets_file(path):\n",
      "    f = open(path)\n",
      "\n",
      "    i = 0\n",
      "    users = []\n",
      "    for line in f:\n",
      "        data = line.split(\"]][[\")\n",
      "        last = len(data)\n",
      "\n",
      "    for i,tweets in enumerate(data):\n",
      "            if i == 0:\n",
      "                t = json.loads(tweets[1:] + \"]\")\n",
      "            elif i == (last-1):\n",
      "                t = json.loads(\"[\"+tweets[:-1])\n",
      "            else:\n",
      "                t = json.loads(\"[\"+tweets+\"]\")\n",
      "            users.append(t)\n",
      "\n",
      "    return users\n",
      "\n",
      "good = get_tweets_file(data_path + \"/good.json\")\n",
      "print \"Real users %s\" % (len(good))\n",
      "     \n",
      "bots = get_tweets_file(data_path + \"/bots.json\")\n",
      "print \"Bot users %s\" % (len(bots))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"total: \", 883+898\n",
      "print \"\\n\".join(sorted(good[0][0].keys()))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"\\n\".join(sorted(good[0][0]['user'].keys()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Distribution of dates from Tweets\n",
      "To avoid bias of the classifier by trending topics the data should belong to the same date range. We want to make sure the users belong to the same period of time. \n",
      "\n",
      "For example, the classifier can learn the topics instead of distinguishing bots from real users. We check for the date distribution of both classes. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime\n",
      "from collections import Counter\n",
      "\n",
      "def get_date(date_str):\n",
      "    return datetime.datetime.strptime(date_str.strip('\"'), \"%a %b %d %H:%M:%S +0000 %Y\")\n",
      "\n",
      "# datetime.strptime((r.json()[x][\"created_at\"]).strip('\"'), \"%a %b %d %H:%M:%S +0000 %Y\")\n",
      "def count_dates(users):\n",
      "    dates = Counter()\n",
      "    min_dt = get_date(users[0][0]['created_at'])\n",
      "    for user in users:\n",
      "        d = get_date(user[0]['created_at'])\n",
      "        min_dt = min(min_dt, d)\n",
      "        dates.update([d])\n",
      "    return dates, min_dt\n",
      "\n",
      "good_counts, min_good = count_dates(good)\n",
      "print \"Most common: %s\" % good_counts.most_common(3)\n",
      "print \"Latest: %s\" % min_good\n",
      "        \n",
      "bots_counts, min_bots = count_dates(bots)\n",
      "print \"Most common: %s\" % bots_counts.most_common(3)\n",
      "print \"Latest: %s\" % min_bots\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Number of users that have old tweets in good users\n",
      "## with tweets not in 2014\n",
      "print \"Old good users %s\" %  len([d for d in good_counts.keys() if d.year < 2014])\n",
      "print \"Old bot users %s\" %  len([d for d in bots_counts.keys() if d.year < 2014])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.dates as mdates\n",
      "import matplotlib as mpl \n",
      "\n",
      "# mpl.style.use('fivethirtyeight')\n",
      "\n",
      "years    = mdates.YearLocator()   # every year\n",
      "months   = mdates.MonthLocator()  # every month\n",
      "yearsFmt = mdates.DateFormatter('%Y')\n",
      "monthsFmt = mdates.DateFormatter('%Y-%m')\n",
      "fig = plt.figure()\n",
      "ax = plt.axes()\n",
      "\n",
      "gds =[(d,c) for d,c in good_counts.iteritems() if d.year > 2013]\n",
      "bts =[(d,c) for d,c in bots_counts.iteritems() if d.year > 2013]\n",
      "\n",
      "\n",
      "kg = [d.toordinal() for d,_ in gds]\n",
      "kb = [d.toordinal() for d,_ in bts]\n",
      "wg = [c for _,c in gds]\n",
      "wb = [c for _,c in bts]\n",
      "plt.hist([kg,kb], weights=[wg,wb], bins=20, stacked=False, alpha=.7, label=['real', 'bots'])\n",
      "\n",
      "ax.xaxis.set_major_locator(months)\n",
      "ax.xaxis.set_major_formatter(monthsFmt)\n",
      "# ax.xaxis.set_minor_locator(months)\n",
      "plt.legend(loc='best', frameon=False, numpoints=1)\n",
      "ax.format_xdata = mdates.DateFormatter('%Y-%m-%d')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Data Processing\n",
      "For each user we have tweet in the timeline. Let $s=\\{t | t \\text{ is a tweet}\\}_{1}^{200}$ be the tweets of a user. Let $D=\\{(s,y)\\}_1^n$, where $y=\\{'human','bot'\\}$. \n",
      "\n",
      "### Text Processing \n",
      "For every user we process the text of the twee as follows:\n",
      "* Collapse URL\n",
      "* Collapse mentions\n",
      "* Lower case all text \n",
      "* Remove uses that have not tweeted this year (i.e., 2014)\n",
      "\n",
      "###Dataset Format\n",
      "We create a dataset dictionary containing: \n",
      "* **data**: All tweet objects per user\n",
      "* **target**: Class label of each user. Labels are 0:humans 1:bots\n",
      "* **user_id**: screen name of the user\n",
      "* **user_name**: full name of the user\n",
      "* **user_text**: all text of tweets converted into one single text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## convert the tweet into a data format of text documents\n",
      "# from sklearn.datasets.base import Bunch\n",
      "import re\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "import numpy as np\n",
      "def preprocess(string, lowercase, collapse_urls, collapse_mentions):\n",
      "    if not string:\n",
      "        return \"\"\n",
      "    if lowercase:\n",
      "        string = string.lower()\n",
      "#     tokens = []\n",
      "    if collapse_urls:\n",
      "        string = re.sub('http\\S+', 'THIS_IS_A_URL', string)\n",
      "    if collapse_mentions:\n",
      "        string = re.sub('@\\S+', 'THIS_IS_A_MENTION', string)\n",
      "#     if prefix:\n",
      "#         tokens = ['%s%s' % (prefix, t) for t in tokens]\n",
      "    return string\n",
      "\n",
      "def timeline_to_doc(user, *args):\n",
      "    tweets = []\n",
      "    for tw in user:\n",
      "        tweets.append(preprocess(tw['text'], *args))\n",
      "    return tweets\n",
      "\n",
      "def user_to_doc(users, *args):\n",
      "    timeline = []\n",
      "    user_names = []\n",
      "    user_id = []\n",
      "    \n",
      "    for user in users:\n",
      "        timeline.append(timeline_to_doc(user, *args))\n",
      "        user_names.append(user[0]['user']['name'])\n",
      "        user_id.append(user[0]['user']['screen_name'])\n",
      "    return user_id, user_names, timeline\n",
      "\n",
      "def bunch_users(class1, class2, vct, *options):\n",
      "    labels = None\n",
      "    if labels is None:\n",
      "        labels = [0,1]\n",
      "\n",
      "    user_id, user_names, timeline = user_to_doc(class1, *options)\n",
      "    user_id2, user_names2, timeline2 = user_to_doc(class2, *options)\n",
      "    target = [labels[0]] * len(user_id)\n",
      "    user_id.extend(user_id2)\n",
      "    user_names.extend(user_names2)\n",
      "    timeline.extend(timeline2)\n",
      "    target.extend([labels[1]]* len(user_id2))\n",
      "    user_text = [\". \".join(t) for  t in timeline]\n",
      "#     data = Bunch(data=timeline, target=target, user_id=user_id, user_name=user_names)\n",
      "    data = {'data':timeline, 'target':np.array(target), 'user_id':user_id, 'user_name':user_names, 'user_text':user_text}\n",
      "    data['bow'] = vct.fit_transform(data['user_text'])\n",
      "\n",
      "    random_state = np.random.RandomState(5612)        \n",
      "\n",
      "    indices = np.arange(data['bow'].shape[0])\n",
      "    random_state.shuffle(indices)\n",
      "    data['target'] = np.array(data['target'])[indices]\n",
      "    data_lst = np.array(data['data'] , dtype=object)\n",
      "    data_lst = data_lst[indices]\n",
      "    data['data'] = data_lst.tolist()\n",
      "    data['bow'] = data['bow'][indices]\n",
      "    data['user_id'] = np.array(data['user_id'])[indices]\n",
      "    data['user_name'] = np.array(data['user_id'])[indices]\n",
      "    data['user_text'] = np.array(data['user_id'])[indices]\n",
      "\n",
      "    return data\n",
      "       "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np \n",
      "vct = TfidfVectorizer(encoding='latin1', min_df=5, max_df=1.0, binary=False, ngram_range=(1, 1),\n",
      "                      token_pattern='\\\\b\\\\w+\\\\b') \n",
      "\n",
      "gds =[g for g in good if get_date(g[0]['created_at']).year > 2013]\n",
      "bts =[b for b in bots if get_date(b[0]['created_at']).year > 2013]\n",
      "\n",
      "data = bunch_users(gds,bts, vct, True, False, False)\n",
      "\n",
      "print \"Total data:\", len(data['target'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Learning Curve: Random Sampling Baseline\n",
      "\n",
      "Test the learning curve as a baseline. This curve is of a classifier trained on documents. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import os\n",
      "\n",
      "sys.path.append(os.path.abspath(\"/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\"))\n",
      "\n",
      "from sklearn.learning_curve import learning_curve\n",
      "from sklearn import linear_model\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "# import brewer2mpl\n",
      "from sklearn.cross_validation import StratifiedKFold, cross_val_score, KFold, ShuffleSplit\n",
      "import itertools\n",
      "\n",
      "def get_tableau():\n",
      "    tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),  \n",
      "                 (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),  \n",
      "                 (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),  \n",
      "                 (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),  \n",
      "                 (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]  \n",
      "\n",
      "    # Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.  \n",
      "    for i in range(len(tableau20)):  \n",
      "        r, g, b = tableau20[i]  \n",
      "        tableau20[i] = (r / 255., g / 255., b / 255.)      \n",
      "\n",
      "    return tableau20\n",
      "\n",
      "def learning_curve_tweet(data,clf, sizes=None):\n",
      "\n",
      "    col = get_tableau()\n",
      "    colors_n = itertools.cycle(col)\n",
      "    random_state = np.random.RandomState(56124)\n",
      "    indices = np.arange(data['bow'].shape[0])\n",
      "    random_state.shuffle(indices)\n",
      "    data['target'] = np.array(data['target'])[indices]\n",
      "    data_lst = np.array(data['data'] , dtype=object)\n",
      "    data_lst = data_lst[indices]\n",
      "    data['data'] = data_lst.tolist()\n",
      "    data['bow'] = data['bow'][indices]\n",
      "    try:\n",
      "        data['user_id'] = np.array(data['user_id'])[indices]\n",
      "        data['user_name'] = np.array(data['user_id'])[indices]\n",
      "        data['user_text'] = np.array(data['user_id'])[indices]\n",
      "    except Exception:\n",
      "        pass\n",
      "\n",
      "    kcv = KFold(len(data['target']), n_folds=5, random_state=random_state,shuffle=True)\n",
      "\n",
      "    scoring_fn = 'accuracy'\n",
      "#     print(\"Classifier name:\", clf.__class__.__name__, \"C=\", clf.C)\n",
      "#     print(\"CV data:\", data['bow'])\n",
      "    if sizes is None:\n",
      "        sizes = range(20, 4* len(data['target'])/5, 100)\n",
      "    train_sizes, train_scores, test_scores = learning_curve(\n",
      "        clf, data['bow'], data['target'], train_sizes=sizes, cv=5, scoring=scoring_fn, n_jobs=2)\n",
      "    \n",
      "    current_color = colors_n.next()\n",
      "\n",
      "    train_scores_mean = np.mean(train_scores, axis=1)\n",
      "    train_scores_std = np.std(train_scores, axis=1)\n",
      "    test_scores_mean = np.mean(test_scores, axis=1)\n",
      "    test_scores_std = 1.0 * np.std(test_scores, axis=1) / np.sqrt(5.0)\n",
      "\n",
      "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
      "                     test_scores_mean + test_scores_std, alpha=0.1, color=current_color)\n",
      "    plt.plot(train_sizes, test_scores_mean, 'o-', mfc='white', linewidth=2, mew=2, markersize=10, mec=current_color, color=current_color,\n",
      "             # label=\"Cross-validation score\")\n",
      "             label=\"C={}\".format(clf.C))\n",
      "\n",
      "    print (\"-\"*40)\n",
      "    print (\"\\nCOST\\tMEAN\\tSTDEV\")\n",
      "    print (\"\\n\".join([\"{0}\\t{1:.3f}\\t{2:.4f}\".format(c,m,s) for c,m,s in zip(train_sizes, test_scores_mean, test_scores_std)]))\n",
      "    plt.legend(loc=\"best\")\n",
      "    # plt.savefig('lr-{0}.png'.format(vct.__class__.__name__), bbox_inches=\"tight\", dpi=200, transparent=True)\n",
      "    plt.savefig('lradapt-sent-sent.png', bbox_inches=\"tight\", dpi=200, transparent=True)\n",
      "    plt.show()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = 'lr'\n",
      "if classifier == \"mnb\":\n",
      "    clf = MultinomialNB(alpha=1)\n",
      "else:\n",
      "    clf = linear_model.LogisticRegression(penalty='l1', C=10)\n",
      "\n",
      "learning_curve_tweet(data,clf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "global name 'random_state' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-14-6da94ffeed5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlearning_curve_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-13-5a61164a57f7>\u001b[0m in \u001b[0;36mlearning_curve_tweet\u001b[0;34m(data, clf, sizes)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bow'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdata_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: global name 'random_state' is not defined"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Preprocessing Options\n",
      "The following results show how the preprocessing of text affects the classification\n",
      "\n",
      "###Results\n",
      "**Text Processing**\n",
      "We observe that there are not significant in the options. However, the best options are:\n",
      "* lower=True\turl=True\tmention=True\n",
      "* lower=True\turl=False\tmention=True\n",
      "* lower=False\turl=True\tmention=True\n",
      "* lower=False\turl=False\tmention=True\n",
      "\n",
      "This seems to suggest that collapsing mentions helps more than other processing options.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## try all combinations of data\n",
      "\n",
      "def try_all(clf, vct, good, bots):\n",
      "    # Trying all possible options \n",
      "    lowercase_opts = [True, False]\n",
      "    # keep_punctuation_opts = [True, False]\n",
      "    url_opts = [True, False]\n",
      "    mention_opts = [True, False]\n",
      "\n",
      "    argnames = ['lower', 'url', 'mention']\n",
      "    option_iter = itertools.product( lowercase_opts,\n",
      "                           url_opts,\n",
      "                           mention_opts)\n",
      "    results = []\n",
      "    for options in option_iter:\n",
      "        print '\\t'.join('%s=%s' % (name, opt) for name, opt in zip(argnames, options))\n",
      "        data = bunch_users(good, bots, vct, *options)\n",
      "        cv_scores = cross_val_score(clf, data['bow'], data['target'], cv=5, n_jobs=1)\n",
      "        print(\"5-f CV Accuracy: %0.2f (+/- %0.2f)\" % (cv_scores.mean(), cv_scores.std() * 2))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = linear_model.LogisticRegression(penalty='l1', C=10)\n",
      "\n",
      "vct = TfidfVectorizer(encoding='latin1', min_df=5, max_df=1.0, binary=False, ngram_range=(1,1),\n",
      "                      token_pattern='\\\\b\\\\w+\\\\b') \n",
      "\n",
      "try_all(clf, vct, gds, bts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_features(coef, names):\n",
      "    \"\"\" Print sorted list of non-zero features/weights. \"\"\"\n",
      "    ### coef = clf.coef_[0]\n",
      "    ### names = vec.get_feature_names()\n",
      "    print \"*\" * 50\n",
      "    print(\"Number of Features: %s\" % len(names))\n",
      "    print \"\\n\".join('%s\\t%.2f' % (names[j], coef[j]) for j in np.argsort(coef)[::-1] if coef[j] != 0)\n",
      "    print \"*\" * 50\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = linear_model.LogisticRegression(penalty='l1', C=10)\n",
      "data = bunch_users(gds, bts, vct, True, True, True)\n",
      "clf.fit(data['bow'], data['target'])\n",
      "print_features(clf.coef_[0], vct.get_feature_names())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Classifier Parameters: Grid Search\n",
      "\n",
      "For the classifier of choice we look for the best possible paramter to maximize accuracy. For logistic regression we look for the C penalty of regularization.\n",
      "\n",
      "### Results: \n",
      "We found that C=10 seems to work well with the data. \n",
      "\n",
      "*Note:* The data only includes users with tweets in the current year. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Grid search best estimator\n",
      "from sklearn.cross_validation import StratifiedKFold, train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "def grid_search_clf(data):\n",
      "    tuned_parameters = [{'C':  [pow(10,x) for x in range(-3,4)]}]\n",
      "    # scores = ['accuracy','precision', 'recall']\n",
      "    measure = 'accuracy'\n",
      "    X_train, X_test, y_train, y_test = train_test_split(data['bow'], data['target'], test_size=0.25, random_state=0)\n",
      "    kcv = StratifiedKFold(y=y_train, n_folds=5, shuffle=True, random_state=546321)\n",
      "    \n",
      "    print(\"# Tuning hyper-parameters for %s\" % measure)\n",
      "    print(len(y_train))\n",
      "    clf_new = linear_model.LogisticRegression(penalty='l1')\n",
      "    clf = GridSearchCV(clf_new, tuned_parameters, cv=5, scoring=measure, n_jobs=10)\n",
      "    clf.fit(X_train, y_train)\n",
      "\n",
      "    print(\"Best parameters set found on development set:\")\n",
      "    print(clf.best_estimator_)\n",
      "    print()\n",
      "    print(\"Grid scores on development set:\")\n",
      "    print()\n",
      "    for params, mean_score, scores in clf.grid_scores_:\n",
      "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
      "              % (mean_score, scores.std() / 2, params))\n",
      "    print()\n",
      "\n",
      "    print(\"Detailed classification report:\")\n",
      "    print()\n",
      "    print(\"The model is trained on the full development set.\")\n",
      "    print(\"The scores are computed on the full evaluation set.\")\n",
      "    print()\n",
      "    y_true, y_pred = y_test, clf.predict(X_test)\n",
      "    print(classification_report(y_true, y_pred))\n",
      "    print()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grid_search_clf(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Data Representation\n",
      "We test the effect of the data representation in the performance of the classifier.\n",
      "\n",
      "###Results\n",
      "We observe that the performance of the classifier does not increase significantly. \n",
      "\n",
      "**Data Representation**\n",
      "\n",
      "We tried two vectorizers: Count and TFIdf. We combined with unigrams and two-grams. \n",
      "\n",
      "TFIDF + one-grams and two-grams work better. \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vct = TfidfVectorizer(encoding='latin1', min_df=5, max_df=1.0, binary=False, ngram_range=(1,2),\n",
      "                      token_pattern='\\\\b\\\\w+\\\\b') \n",
      "\n",
      "try_all(clf, vct, gds, bts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vct = TfidfVectorizer(encoding='latin1', min_df=1, max_df=1.0, binary=False, ngram_range=(1,1),\n",
      "                      token_pattern='\\\\b\\\\w+\\\\b') \n",
      "\n",
      "try_all(clf, vct, gds, bts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vct = CountVectorizer(encoding='latin1', min_df=5, max_df=1.0, binary=False, ngram_range=(1,1),\n",
      "                      token_pattern='\\\\b\\\\w+\\\\b') \n",
      "\n",
      "try_all(clf, vct, gds, bts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Testing Sentence Classifier\n",
      "Test how does a sentence classifier do classifying into bots and humans."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##sentence detector in tweets\n",
      "def sentence_detector(timeline):\n",
      "    tl = []\n",
      "    print len(timeline)\n",
      "    for tw in timeline:\n",
      "        tl.append(tw)\n",
      "    return tl\n",
      "\n",
      "def convert2sentence(data):\n",
      "    all_sent=[]\n",
      "    all_target=[]\n",
      "    for user_timeline, label in zip(data['data'], data['target']):\n",
      "        sentences = user_timeline\n",
      "        lbls = [label] * len(sentences)\n",
      "        all_sent.extend(sentences)\n",
      "        all_target.extend(lbls)\n",
      "    return all_sent, all_target\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vct = TfidfVectorizer(encoding='latin1', min_df=5, max_df=1.0, binary=False, ngram_range=(1,2),\n",
      "                      token_pattern='\\\\b\\\\w+\\\\b') \n",
      "x_sent,y_sent = convert2sentence(data)\n",
      "\n",
      "indices = np.arange(len(y_sent))\n",
      "random_state.shuffle(indices)\n",
      "y_sent = np.array(y_sent)[indices]\n",
      "data_lst = np.array(x_sent , dtype=object)\n",
      "data_lst = data_lst[indices]\n",
      "x_sent = data_lst.tolist()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_sent = linear_model.LogisticRegression(penalty='l1', C=10)\n",
      "print \"Total sentences:\", len(y_sent)\n",
      "learning_curve_tweet({'data':x_sent, 'target':y_sent, 'bow':vct.fit_transform(x_sent)},clf, sizes=range(1000, 20000, 1000))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Other Features\n",
      "Term uniqueness? \n",
      "\n",
      "%%latex\n",
      "\\begin{align}\n",
      "f(x) = \\frac{TF}{#UniqueTerms}\n",
      "\\begin{align}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}